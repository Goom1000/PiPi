---
phase: 36-core-ask-ai
plan: 02
type: execute
wave: 2
depends_on: ["36-01"]
files_modified:
  - services/geminiService.ts
  - services/providers/geminiProvider.ts
autonomous: true

must_haves:
  truths:
    - "Gemini provider streamChat yields text chunks from generateContentStream"
    - "System prompt instructs age-appropriate language based on gradeLevel"
    - "Streaming works with for-await-of in consumer"
  artifacts:
    - path: "services/geminiService.ts"
      provides: "streamChatResponse async generator function"
      exports: ["streamChatResponse"]
    - path: "services/providers/geminiProvider.ts"
      provides: "streamChat method implementation"
  key_links:
    - from: "services/providers/geminiProvider.ts"
      to: "services/geminiService.ts"
      via: "streamChat calls streamChatResponse"
      pattern: "streamChatResponse"
    - from: "services/geminiService.ts"
      to: "@google/genai"
      via: "generateContentStream API"
      pattern: "generateContentStream"
---

<objective>
Implement streaming chat for Gemini provider using generateContentStream API.

Purpose: Enable Ask AI to stream responses from Gemini, yielding text chunks as they arrive from the API.

Output: streamChatResponse function in geminiService.ts and streamChat wrapper in geminiProvider.ts.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/36-core-ask-ai/36-RESEARCH.md
@.planning/phases/36-core-ask-ai/36-01-SUMMARY.md

@services/aiProvider.ts
@services/geminiService.ts
@services/providers/geminiProvider.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement streamChatResponse in geminiService.ts</name>
  <files>services/geminiService.ts</files>
  <action>
Add streamChatResponse async generator function. Place it near other generation functions.

```typescript
import { ChatContext } from './aiProvider';

/**
 * Stream a chat response from Gemini using generateContentStream.
 * Yields text chunks as they arrive.
 */
export async function* streamChatResponse(
  apiKey: string,
  message: string,
  context: ChatContext
): AsyncGenerator<string, void, unknown> {
  const ai = new GoogleGenAI({ apiKey });

  const systemPrompt = `You are a helpful teaching assistant. The teacher is presenting a lesson to ${context.gradeLevel} students.

CURRENT LESSON CONTEXT:
- Topic: ${context.lessonTopic}
- Current Slide: ${context.currentSlideTitle}
- Slide Content: ${context.currentSlideContent.join('; ')}

INSTRUCTIONS:
- Give clear, helpful answers to the teacher's question
- Use language appropriate for ${context.gradeLevel} students
- Be concise but thorough
- If relevant, reference the lesson content
- Do NOT include markdown formatting (no **, no ##, no bullet points)
- Write in plain, conversational prose`;

  const response = await ai.models.generateContentStream({
    model: 'gemini-2.0-flash-exp',
    contents: message,
    config: {
      systemInstruction: systemPrompt,
    },
  });

  for await (const chunk of response) {
    if (chunk.text) {
      yield chunk.text;
    }
  }
}
```

Also add the import for ChatContext at the top of the file if not already present:
```typescript
import { ChatContext } from './aiProvider';
```

Export the function (add to exports if using explicit exports, or ensure it's accessible as a named export).
  </action>
  <verify>
TypeScript compiles:
```bash
npx tsc --noEmit 2>&1 | grep -E "(geminiService|error)" | head -10
```

Function exists:
```bash
grep -n "streamChatResponse" services/geminiService.ts
```
  </verify>
  <done>streamChatResponse async generator function exists in geminiService.ts, yields text chunks from Gemini streaming API.</done>
</task>

<task type="auto">
  <name>Task 2: Add streamChat to GeminiProvider</name>
  <files>services/providers/geminiProvider.ts</files>
  <action>
Import streamChatResponse from geminiService and ChatContext from aiProvider:

```typescript
import {
  // ... existing imports
  streamChatResponse as geminiStreamChatResponse,
} from '../geminiService';

import {
  // ... existing imports
  ChatContext,
} from '../aiProvider';
```

Add streamChat method to GeminiProvider class (after regenerateTeleprompter method):

```typescript
  async *streamChat(
    message: string,
    context: ChatContext
  ): AsyncGenerator<string, void, unknown> {
    try {
      yield* geminiStreamChatResponse(this.apiKey, message, context);
    } catch (error) {
      throw this.wrapError(error);
    }
  }
```

Note the `async *` syntax for async generator methods and `yield*` to delegate to the underlying generator.
  </action>
  <verify>
TypeScript compiles without errors for geminiProvider:
```bash
npx tsc --noEmit 2>&1 | grep -E "geminiProvider" | head -5
```

streamChat method exists:
```bash
grep -n "streamChat" services/providers/geminiProvider.ts
```
  </verify>
  <done>GeminiProvider.streamChat method implemented, delegating to geminiStreamChatResponse.</done>
</task>

</tasks>

<verification>
After both tasks complete:

1. TypeScript compiles for gemini files:
   ```bash
   npx tsc --noEmit 2>&1 | grep -E "(gemini|error)" | head -10
   ```

2. streamChatResponse is exported from geminiService:
   ```bash
   grep -n "export.*streamChatResponse" services/geminiService.ts
   ```

3. GeminiProvider implements streamChat:
   ```bash
   grep -A5 "streamChat" services/providers/geminiProvider.ts
   ```

4. ChatContext import present in both files:
   ```bash
   grep "ChatContext" services/geminiService.ts services/providers/geminiProvider.ts
   ```
</verification>

<success_criteria>
- streamChatResponse function in geminiService.ts yields text chunks
- System prompt includes gradeLevel for age-appropriate responses (CTXT-02)
- System prompt includes lesson context (CTXT-01)
- GeminiProvider.streamChat wraps streamChatResponse with error handling
- TypeScript compiles without errors for these files
</success_criteria>

<output>
After completion, create `.planning/phases/36-core-ask-ai/36-02-SUMMARY.md`
</output>
